import { scrapeNewsContent } from '../lib/scraper/newsContent'

async function testScraping() {
  const testUrl = 'https://www.mt.co.kr/living/2026/01/16/2026011615415560115'

  console.log('='.repeat(80))
  console.log('ğŸ” ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ì‹œì‘')
  console.log('URL:', testUrl)
  console.log('='.repeat(80))

  try {
    const content = await scrapeNewsContent(testUrl, 5000)

    console.log('\nâœ… ìŠ¤í¬ë˜í•‘ ì„±ê³µ!')
    console.log(`ğŸ“Š í™•ë³´í•œ ê¸€ì ìˆ˜: ${content.length}ì`)
    console.log('\nğŸ“° í™•ë³´í•œ ë‚´ìš© (ì²˜ìŒ 500ì):')
    console.log('-'.repeat(80))
    console.log(content.substring(0, 500))
    console.log('-'.repeat(80))

    if (content.length >= 1000) {
      console.log('\nğŸ“° ì¤‘ê°„ ë¶€ë¶„ (1000-1500ì):')
      console.log('-'.repeat(80))
      console.log(content.substring(1000, 1500))
      console.log('-'.repeat(80))
    }

  } catch (error) {
    console.error('\nâŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨:', error)
  }
}

testScraping()
